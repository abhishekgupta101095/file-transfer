from airflow import DAG
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.microsoft.azure.hooks.wasb import WasbHook
from datetime import datetime, timedelta
import airflow.utils.dates

import os
from pathlib import Path

from airflow.providers.google.cloud.transfers.gcs_to_local import GCSToLocalFilesystemOperator
from airflow.utils.trigger_rule import TriggerRule

from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

import scripts.config as config

from scripts.s3_processing_python import process_s3
from scripts.DataQualityChecks import dataQualityChecksProcess

import boto3
from scripts.invidual_data_ingestion_3p import individual_process_3p
from airflow.operators.python import BranchPythonOperator

from scripts.segment_demo_insights_view_ingestion import segment_demo_insights_view_process
from scripts.segment_profile_web import segment_profile_web_view_process
from scripts.segment_profile_poi import segment_profile_poi_view_process

from airflow.sensors.external_task_sensor import ExternalTaskSensor
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

BUCKET_NAME=config.bucketName
METADATA_LOCAL_PATH=config.locatPathToDownloadDictionaryFiles
S3_DICTIONARY_PREFIX=config.s3DictionaryPrefix
S3_DELIMITER=config.s3Delimiter
AWS_CONN_ID=config.awsConnId

BUCKET_NAME=config.bucketName
RAWDATA_LOCAL_PATH=config.locatPathToDownloadRawFiles
S3_RAW_FILES_PREFIX=config.s3RawFilesPrefix
S3_DELIMITER=config.s3Delimiter
AWS_CONN_ID=config.awsConnId
aws_access_key_id=config.aws_access_key_id
aws_secret_access_key=config.aws_secret_access_key
aws_region=config.aws_region
s3_processed_path=config.s3_processed_path

default_args = {
        "owner": "airflow", 
        "start_date": airflow.utils.dates.days_ago(1)
    }

    
def download_from_s3(key: str, s3_prefix: str, localFilePath: str) -> None:
    hook = S3Hook('aws_udm')
    if key == s3_prefix:
        return None
    file_name = hook.download_file(
        key=key,
        bucket_name=BUCKET_NAME,
        preserve_file_name=True,
        local_path=localFilePath,
        use_autogenerated_subdir=False
    )
    # will return absolute path
    return file_name


def move_s3_objects(key: str, s3_prefix: str) -> None:
    if key == s3_prefix:
        return None
    s3 = boto3.client(
        's3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=aws_region
    )
    current_date = datetime.now()
    formatted_date = current_date.strftime('%Y%m%d')
    dirPath = key.rsplit('/', 1)[0]
    fileName = key.rsplit('/', 1)[1]
    newKey = dirPath + '/' + formatted_date + '/' + fileName
    destination_key = newKey.replace(S3_RAW_FILES_PREFIX, s3_processed_path, 1)
    # Copy the object to the new location
    s3.copy_object(
            Bucket=BUCKET_NAME,
            CopySource={'Bucket': BUCKET_NAME, 'Key': key},
            Key=destination_key
        )


def delete_s3_objects(key: str, s3_prefix: str) -> None:
    if key == s3_prefix:
        return None
    s3 = boto3.client(
        's3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=aws_region
    )
    s3.delete_object(Bucket=BUCKET_NAME, Key=key)


default_args = {
        "owner": "airflow", 
        "start_date": airflow.utils.dates.days_ago(1)
    }


def folder_exists(**kwargs):
    current_date = datetime.now()
    formatted_date = current_date.strftime('%Y%m%d')
    path=s3_processed_path+formatted_date
    ti = kwargs['ti']
    s3 = boto3.client(
        's3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=aws_region
    )
    #path = 'CDI/data/raw/20240214'.rstrip('/') 
    resp = s3.list_objects(Bucket=BUCKET_NAME, Prefix=path, Delimiter=S3_DELIMITER,MaxKeys=1)
    if 'CommonPrefixes' in resp:
        ti.xcom_push(key="pushed_value", value='directory_exists')
    else:
        ti.xcom_push(key="pushed_value", value='directory_does_not_exists')


def create_directory(**kwargs):
    pushed_value=5
    current_date = datetime.now()
    formatted_date = current_date.strftime('%Y%m%d')
    directory_path=s3_processed_path+formatted_date
    if not directory_path.endswith('/'):
        directory_path += '/'
    s3 = boto3.client(
        's3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=aws_region
    ) 
    s3.put_object(Bucket=BUCKET_NAME, Key=directory_path)


def branch_function(**kwargs):
    ti = kwargs['ti']
    pulled_value = ti.xcom_pull(key='pushed_value', task_ids='check_folder_exist_task')
    if pulled_value == 'directory_exists':
        return 'hop_task'
    else:
        return 'create_directory_task'


with DAG(dag_id="load3pIndividual", default_args=default_args, schedule_interval=None) as dag:
    
    '''individual3PJobSensor = ExternalTaskSensor(
        task_id='individual3PJobSensor_task',
        external_dag_id='loadMetadata',
        external_task_id='end',
        poke_interval=5,
        timeout=60 * 10
       )'''
    
    start = DummyOperator(
    task_id='start'
    )
    
    listRawFilesS3 = S3ListOperator(
        task_id="listRawFilesS3",
        bucket=BUCKET_NAME,
        prefix=S3_RAW_FILES_PREFIX,
        delimiter=S3_DELIMITER,
        aws_conn_id=AWS_CONN_ID
    )
           
    individual_ingestion_3p = PythonOperator(
        task_id='individual_ingestion_3p',
        python_callable=individual_process_3p
    )
    
    
    check_folder_exist_task = PythonOperator(task_id='check_folder_exist_task', python_callable=folder_exists, provide_context=True)
    branch_task = BranchPythonOperator(task_id='branch_task', python_callable=branch_function, provide_context=True)
    
    hop_task = DummyOperator(
    task_id='hop_task'
    )
    
    create_directory_task = PythonOperator(
    task_id='create_directory_task', 
    python_callable=create_directory
    )
              
    move_files_task = PythonOperator.partial(
        task_id='move_files_task',
        trigger_rule='none_failed',
        python_callable=move_s3_objects
        ).expand(
          op_kwargs=listRawFilesS3.output.map(
            lambda x: {
                "key":f"{x}",
                "s3_prefix":S3_RAW_FILES_PREFIX
            }
        )
    )


    delete_s3_objects = PythonOperator.partial(
        task_id='delete_s3_objects',
        trigger_rule='none_failed',
        python_callable=delete_s3_objects
        ).expand(
          op_kwargs=listRawFilesS3.output.map(
            lambda x: {
                "key":f"{x}",
                "s3_prefix":S3_RAW_FILES_PREFIX
            }
        )
    )
    
    triggerCreate3PViews = TriggerDagRunOperator(
        task_id="triggerCreate3PViews",
        trigger_dag_id="create3PViews"
        #execution_date=EXEC_DATE
    )
    
    end = DummyOperator(
    task_id='end'
    )

    #load_data_intopostgres = BashOperator(
    #    task_id="load_data_intopostgres",
    #    bash_command="python3 /opt/airflow/dags/scripts/postgreswrite.py"
    #)

start >> [listRawFilesS3,individual_ingestion_3p] >> check_folder_exist_task
check_folder_exist_task >> branch_task >> [hop_task, create_directory_task] >> move_files_task >> delete_s3_objects >> triggerCreate3PViews >> end
